{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ML Quiz\n",
    "* 문제: 맨 아래 셀에 코드를 작성하여 predict_test dataframe에서 Outcome과 prediction이 일치하는 샘플의 비율, 즉 정확도를 계산하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my app\").master(\"local\").getOrCreate()\n",
    "\n",
    "# get context from the session\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "일단 그냥 읽어봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pregnancies',\n",
       " 'Glucose',\n",
       " 'BloodPressure',\n",
       " 'SkinThickness',\n",
       " 'Insulin',\n",
       " 'BMI',\n",
       " 'DiabetesPedigreeFunction',\n",
       " 'Age',\n",
       " 'Outcome']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"diabetes.csv\")\n",
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pregnancies: string (nullable = true)\n",
      " |-- Glucose: string (nullable = true)\n",
      " |-- BloodPressure: string (nullable = true)\n",
      " |-- SkinThickness: string (nullable = true)\n",
      " |-- Insulin: string (nullable = true)\n",
      " |-- BMI: string (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Outcome: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
      "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|\n",
      "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "schema를 지정하여 읽음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('Pregnancies',FloatType(),True),\n",
    "    StructField('Glucose',FloatType(),True),\n",
    "    StructField('BloodPressure',FloatType(),True),\n",
    "    StructField('SkinThickness',FloatType(),True),\n",
    "    StructField('Insulin',FloatType(),True),\n",
    "    StructField('BMI',FloatType(),True),\n",
    "    StructField('DiabetesPedigreeFunction',FloatType(),True),\n",
    "    StructField('Age',IntegerType(),True),\n",
    "    StructField('Outcome',IntegerType(),True)\n",
    "])\n",
    "\n",
    "raw_data=spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pregnancies: float (nullable = true)\n",
      " |-- Glucose: float (nullable = true)\n",
      " |-- BloodPressure: float (nullable = true)\n",
      " |-- SkinThickness: float (nullable = true)\n",
      " |-- Insulin: float (nullable = true)\n",
      " |-- BMI: float (nullable = true)\n",
      " |-- DiabetesPedigreeFunction: float (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Outcome: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### describe(*cols)\n",
    "Computes basic statistics for numeric and string columns.\n",
    "\n",
    "This include count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns.\n",
    "\n",
    "Note This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------------+------------------+------------------+\n",
      "|summary|       Pregnancies|          Glucose|     BloodPressure|     SkinThickness|           Insulin|              BMI|DiabetesPedigreeFunction|               Age|           Outcome|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------------+------------------+------------------+\n",
      "|  count|               768|              768|               768|               768|               768|              768|                     768|               768|               768|\n",
      "|   mean|3.8450520833333335|     120.89453125|       69.10546875|20.536458333333332| 79.79947916666667|31.99257813890775|      0.4718763029280429|33.240885416666664|0.3489583333333333|\n",
      "| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|15.952217567727642|115.24400235133803|7.884160293010772|      0.3313285967924436|11.760231540678689| 0.476951377242799|\n",
      "|    min|               0.0|              0.0|               0.0|               0.0|               0.0|              0.0|                   0.078|                21|                 0|\n",
      "|    max|              17.0|            199.0|             122.0|              99.0|             846.0|             67.1|                    2.42|                81|                 1|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|          Glucose|     BloodPressure|     SkinThickness|           Insulin|              BMI|\n",
      "+-------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|              768|               768|               768|               768|              768|\n",
      "|   mean|     120.89453125|       69.10546875|20.536458333333332| 79.79947916666667|31.99257813890775|\n",
      "| stddev|31.97261819513622|19.355807170644777|15.952217567727642|115.24400235133803|7.884160293010772|\n",
      "|    min|              0.0|               0.0|               0.0|               0.0|              0.0|\n",
      "|    max|            199.0|             122.0|              99.0|             846.0|             67.1|\n",
      "+-------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.describe('Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary(*statistics)\n",
    "Computes specified statistics for numeric and string columns. Available statistics are: - count - mean - stddev - min - max - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n",
    "\n",
    "If no statistics are given, this function computes count, mean, stddev, min, approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
    "\n",
    "Note This function is meant for exploratory data analysis, as we make no guarantee about the backward compatibility of the schema of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|          Glucose|     BloodPressure|     SkinThickness|           Insulin|              BMI|\n",
      "+-------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|              768|               768|               768|               768|              768|\n",
      "|   mean|     120.89453125|       69.10546875|20.536458333333332| 79.79947916666667|31.99257813890775|\n",
      "| stddev|31.97261819513622|19.355807170644777|15.952217567727642|115.24400235133803|7.884160293010772|\n",
      "|    min|              0.0|               0.0|               0.0|               0.0|              0.0|\n",
      "|    25%|             99.0|              62.0|               0.0|               0.0|             27.3|\n",
      "|    50%|            117.0|              72.0|              23.0|              29.0|             32.0|\n",
      "|    75%|            140.0|              80.0|              32.0|             127.0|             36.6|\n",
      "|    max|            199.0|             122.0|              99.0|             846.0|             67.1|\n",
      "+-------+-----------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select('Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측 값을 하나라도 갖는 row의 개수 카운트 하기\n",
    "\n",
    "결측값이 None으로 표현되어 있는지 확인해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.rdd.map(\n",
    "    lambda row: 1 if sum([c == None for c in row]) > 0 else 0\n",
    ").reduce(\n",
    "    lambda x, y: x+y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측 값을 하나라도 갖는 row의 개수 카운트 하기\n",
    "\n",
    "0으로 표현된 결측값을 하나라도 갖는 row의 개수 세어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "raw_data.rdd.map(\n",
    "    lambda row: 1 if sum([row[c] == 0 for c in prep_cols]) > 0 else 0\n",
    ").reduce(\n",
    "    lambda x, y: x+y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null zero값을 None으로 치환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "prep_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "for c in prep_cols:\n",
    "    raw_data = raw_data.withColumn(c, fn.when(fn.col(c) == 0, None).otherwise(fn.col(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "|        6.0|  148.0|         72.0|         35.0|   null|33.6|                   0.627| 50|      1|\n",
      "|        1.0|   85.0|         66.0|         29.0|   null|26.6|                   0.351| 31|      0|\n",
      "|        8.0|  183.0|         64.0|         null|   null|23.3|                   0.672| 32|      1|\n",
      "|        1.0|   89.0|         66.0|         23.0|   94.0|28.1|                   0.167| 21|      0|\n",
      "|        0.0|  137.0|         40.0|         35.0|  168.0|43.1|                   2.288| 33|      1|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 별로 결측값의 비율 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------+---------------------+------------------+------------------+\n",
      "|   Glucose_missing|BloodPressure_missing|SkinThickness_missing|   Insulin_missing|       BMI_missing|\n",
      "+------------------+---------------------+---------------------+------------------+------------------+\n",
      "|0.9934895833333334|   0.9544270833333334|   0.7044270833333334|0.5130208333333334|0.9856770833333334|\n",
      "+------------------+---------------------+---------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "prep_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "raw_data.select(*[\n",
    "    (fn.count(c) / fn.count('*')).alias(c + '_missing') for c in prep_cols\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+---------+----+------------------------+---+-------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|  Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
      "+-----------+-------+-------------+-------------+---------+----+------------------------+---+-------+\n",
      "|        6.0|  148.0|         72.0|         35.0|155.54822|33.6|                   0.627| 50|      1|\n",
      "|        1.0|   85.0|         66.0|         29.0|155.54822|26.6|                   0.351| 31|      0|\n",
      "|        8.0|  183.0|         64.0|     29.15342|155.54822|23.3|                   0.672| 32|      1|\n",
      "|        1.0|   89.0|         66.0|         23.0|     94.0|28.1|                   0.167| 21|      0|\n",
      "|        0.0|  137.0|         40.0|         35.0|    168.0|43.1|                   2.288| 33|      1|\n",
      "+-----------+-------+-------------+-------------+---------+----+------------------------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "prep_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "imputer=Imputer(inputCols=prep_cols,outputCols=prep_cols)\n",
    "model=imputer.fit(raw_data)\n",
    "raw_data=model.transform(raw_data)\n",
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 분석을 위한 준비 1. Feature vector생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측해야할 class 속성 값 Outcome을 제외하고 나머지가 feature\n",
    "cols=raw_data.columns\n",
    "cols.remove(\"Outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler를 이용해 dataframe의 row를 vector로 변환\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
    "raw_data=assembler.transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------+\n",
      "|features                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "|[6.0,148.0,72.0,35.0,155.5482177734375,33.599998474121094,0.6269999742507935,50.0]             |\n",
      "|[1.0,85.0,66.0,29.0,155.5482177734375,26.600000381469727,0.35100001096725464,31.0]             |\n",
      "|[8.0,183.0,64.0,29.153419494628906,155.5482177734375,23.299999237060547,0.671999990940094,32.0]|\n",
      "|[1.0,89.0,66.0,23.0,94.0,28.100000381469727,0.16699999570846558,21.0]                          |\n",
      "|[0.0,137.0,40.0,35.0,168.0,43.099998474121094,2.2880001068115234,33.0]                         |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.select(\"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 분석을 위한 준비 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[6.0,148.0,72.0,3...|[1.78063837321943...|\n",
      "|[1.0,85.0,66.0,29...|[0.29677306220323...|\n",
      "|[8.0,183.0,64.0,2...|[2.37418449762590...|\n",
      "|[1.0,89.0,66.0,23...|[0.29677306220323...|\n",
      "|[0.0,137.0,40.0,3...|[0.0,4.5012560836...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardscaler=StandardScaler(\n",
    "    inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "raw_data=standardscaler.fit(raw_data).transform(raw_data)\n",
    "raw_data.select(\"features\",\"scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 분석을 위한 준비 3. Train data set과 Test data set 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = raw_data.randomSplit([0.8, 0.2], seed=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Outcome|prediction|\n",
      "+-------+----------+\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "|      1|       0.0|\n",
      "|      0|       0.0|\n",
      "|      0|       0.0|\n",
      "+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"Outcome\",\n",
    "    featuresCol=\"scaled_features\",\n",
    "    maxIter=10\n",
    ")\n",
    "model=lr.fit(train)\n",
    "predict_train=model.transform(train)\n",
    "predict_test=model.transform(test)\n",
    "predict_test.select(\"Outcome\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------+--------------------------------------+----------+\n",
      "|Outcome|rawPrediction                           |probability                           |prediction|\n",
      "+-------+----------------------------------------+--------------------------------------+----------+\n",
      "|0      |[2.9514000261074718,-2.9514000261074718]|[0.95032961588646,0.04967038411353985]|0.0       |\n",
      "+-------+----------------------------------------+--------------------------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.select(\"Outcome\",\"rawPrediction\", \"probability\", \"prediction\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래 셀에 predict_test의 예측결과의 정확도를 계산하는 코드를 넣으시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.78125\n"
     ]
    }
   ],
   "source": [
    "hit = predict_test.rdd.map(lambda row: 1 if row['Outcome'] == row['prediction'] else 0).reduce(lambda x, y: x+y)\n",
    "testsize = predict_test.count()\n",
    "print(\"precision = {}\".format(hit / testsize))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
