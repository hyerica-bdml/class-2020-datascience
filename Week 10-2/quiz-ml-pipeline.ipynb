{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my app\").master(\"local\").getOrCreate()\n",
    "\n",
    "# get context from the session\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ('This movie was so poorly written and directed I fell asleep 30 minutes through the movie.', 0),\n",
    "    ('The most interesting thing about Miryang (Secret Sunshine) is the actors.', 1),\n",
    "    ('William Hurt may not be an American matinee idol anymore, but he still has pretty good taste in B-movie.', 1)\n",
    "], ['text', 'sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                               words|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[this, movie, was, so, poorly, written, and, directed, i, fell, asleep, 30, minutes, through, the...|\n",
      "|               [the, most, interesting, thing, about, miryang, (secret, sunshine), is, the, actors.]|\n",
      "|[william, hurt, may, not, be, an, american, matinee, idol, anymore,, but, he, still, has, pretty,...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "df.select(\"words\").show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords and special chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                               clean_words|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|                      [movie, poorly, written, directed, fell, asleep, 30, minutes, movie]|\n",
      "|                                   [interesting, thing, miryang, secret, sunshine, actors]|\n",
      "|[william, hurt, may, american, matinee, idol, anymore, still, pretty, good, taste, bmovie]|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def f(s):\n",
    "    return [ ''.join(e for e in token if e.isalnum()) for token in s if token not in stopwords ]\n",
    "\n",
    "func = udf(f, ArrayType(StringType()))\n",
    "df = df.withColumn('clean_words', func(df['words']))\n",
    "df.select(\"clean_words\").show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                      tf|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "|                      (262144,[52800,98627,103409,152575,155321,223227,249111,262048],[1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0])|\n",
      "|                                            (262144,[46512,86875,107499,237079,240227,252717],[1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(262144,[6258,24817,36200,68474,101702,113432,121981,138836,140586,164508,175449,181389],[1.0,1.0,1.0,1.0,1.0,1.0,1.0...|\n",
      "+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"clean_words\", outputCol=\"tf\")\n",
    "df = hashingTF.transform(df)\n",
    "df.select(\"tf\").show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------------+\n",
      "|word       |vector                                       |\n",
      "+-----------+---------------------------------------------+\n",
      "|interesting|[-0.1858113706111908,-0.10948356240987778]   |\n",
      "|secret     |[0.17072227597236633,0.1150684505701065]     |\n",
      "|asleep     |[0.05812466889619827,-0.1537899225950241]    |\n",
      "|hurt       |[-0.028090475127100945,-0.010738671757280827]|\n",
      "|taste      |[-0.22129961848258972,-0.1893204003572464]   |\n",
      "+-----------+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'VectorAssembler' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "w2v = Word2Vec(vectorSize=2, inputCol=\"clean_words\", outputCol=\"w2v\", minCount=1, maxIter=10)\n",
    "model = w2v.fit(df)\n",
    "model.getVectors().show(5, truncate=False)\n",
    "\n",
    "# create an average word vector for each document (works well according to Zeyu & Shu)\n",
    "df = model.transform(df)\n",
    "df.select(\"w2v\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262146,[52800,98627,103409,152575,155321,223227,249111,262048,262144,262145],[1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,-0.028119811167319615,0.036776250435246356])                                         |\n",
      "|(262146,[46512,86875,107499,237079,240227,252717,262144,262145],[1.0,1.0,1.0,1.0,1.0,1.0,-0.03396759647876024,-0.030281569343060255])                                                               |\n",
      "|(262146,[6258,24817,36200,68474,101702,113432,121981,138836,140586,164508,175449,181389,262144,262145],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-0.002610186308932801,0.028247118229046464])|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "asm = VectorAssembler(inputCols=[hashingTF.getOutputCol(), w2v.getOutputCol()],\n",
    "                      outputCol=\"features\")\n",
    "df = asm.transform(df)\n",
    "df.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "svm = LinearSVC(labelCol=\"sentiment\")\n",
    "df = svm.fit(df).transform(df)\n",
    "df.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class RemoveStopWordsAndSpecialCharacters(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super(RemoveStopWordsAndSpecialCharacters, self).__init__()\n",
    "        self.stopwords = Param(self, \"stopwords\", \"\")\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "\n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "\n",
    "        def f(s):\n",
    "            return [ ''.join(e for e in token if e.isalnum()) for token in s if token not in stopwords ]\n",
    "\n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ('This movie was so poorly written and directed I fell asleep 30 minutes through the movie.', 0),\n",
    "    ('The most interesting thing about Miryang (Secret Sunshine) is the actors.', 1),\n",
    "    ('William Hurt may not be an American matinee idol anymore, but he still has pretty good taste in B-movie.', 1)\n",
    "], ['text', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       1.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, Word2Vec, VectorAssembler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "cleaning = RemoveStopWordsAndSpecialCharacters(inputCol=\"words\", outputCol=\"clean_words\",\n",
    "                                               stopwords=stopwords)\n",
    "hashingTF = HashingTF(inputCol=\"clean_words\", outputCol=\"tf\")\n",
    "w2v = Word2Vec(vectorSize=2, inputCol=\"clean_words\", outputCol=\"w2v\",\n",
    "               minCount=1, maxIter=10)\n",
    "asm = VectorAssembler(inputCols=[hashingTF.getOutputCol(), w2v.getOutputCol()],\n",
    "                      outputCol=\"features\")\n",
    "svm = LinearSVC(labelCol=\"sentiment\")\n",
    "\n",
    "mypipeline = Pipeline(stages=[tokenizer, cleaning, hashingTF, w2v, asm, svm])\n",
    "df = mypipeline.fit(df).transform(df)\n",
    "df.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
